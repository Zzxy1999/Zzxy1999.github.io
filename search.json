[{"title":"6.s081 pgtbl & cow","path":"/2025/01/23/6-s081-pgtbl-cow/","content":"1. 虚拟内存与页表1.1 虚拟内存虚拟内存是os对内存的一种管理技术，它提供给每个进程独立的虚拟地址空间，对进程来说只看得到虚拟地址，在cpu执行内存操作时，MMU把虚拟地址转换为物理地址再进行实际的内存访问。 1.2 页表虚拟内存到物理内存的映射靠的是页表，页表的一个条目叫PTE。如果现在知道了页表，我们就能用它计算出一个虚拟地址对应的物理地址，计算方法大概是索引+地址组合。xv6采用的是3级页表，所以需要计算三轮才可以得到相应的物理地址。 页表是os管理并设置在内存中的，那么MMU是怎么用页表的呢？xv6会把当前的页表根地址放在$satp寄存器，MMU根据该寄存器中的页表地址就可以计算物理地址了。 另外计算机一般都有TLB作为更高层次的缓存，如果虚拟地址命中了TLB，那么就不要到内存一步步查页表了。需要注意的是TLB内用户虚拟地址映射是和进程相关的，所以切换进程时，上一个进程的记录就不能使用了（仅用户空间地址）。 2. 内核内存管理2.1 内核地址空间映射xv6内核态下物理地址和虚拟地址是直接映射的，也就是说虚拟地址和物理地址相同，不过有两个例外： trampoline：trampoline是一段代码，主要用于发生trap是内核态和用户态之间的切换。trampoline加载在物理内存中，分别映射到了内核和用户空间。 guard page：每一个进程虽然都是一个内核空间，但是却有着不同的内核栈，所以为了防止内核栈的溢出，每个内核栈之间都被映射了一段guard page，在PTE上置合适的访问位，进行地址解析的时候就可以防止栈溢出了。 2.2 内核内存管理先说一下之前遇到的坑：xv6的内核栈是有限制的且不能伸缩，所以在内核态下，如果分配了一个较大的局部数组，内核栈会溢出（编译没问题）。一旦程序中访问到了溢出的部分，就会出现page fault，也就是访问到了上节说的guard page。 而xv6内核是没有内存分配器的，所以内核需要分配空间都是直接分一个物理页（很自然的直接映射）然后使用，要想比较精细的使用连续访问内存，就得自己写一个内存分配器。 2.2.1 物理内存分配物理内存用空闲链表组织管理，每个节点为PGSIZE大小，可以看到next信息是存储在每个页内的。 void *kalloc(void);void kfree(void *pa);struct run struct run *next;;struct struct spinlock lock; struct run *freelist; kmem;void *kalloc(void); // 取个链表节点返回void kfree(void *pa); // 释放内存添加到链表 需要申请物理内存或者释放物理内存调用kalloc和kfree即可，这里的分配和释放都是以页为单位的。 2.2.2 添加映射在页表内添加映射函数做的工作如下： typedef uint64 *pagetable_t;intmappages(pagetable_t pagetable, uint64 va, uint64 size, uint64 pa, int perm) uint64 a, last; pte_t *pte; if(size == 0) panic(mappages: size); a = PGROUNDDOWN(va); // 虚拟内存的对齐 last = PGROUNDDOWN(va + size - 1); for(;;) if((pte = walk(pagetable, a, 1)) == 0) // 页表中找到pte return -1; if(*pte PTE_V) panic(mappages: remap); *pte = PA2PTE(pa) | perm | PTE_V; // 物理地址写入pte，设置访问位 if(a == last) break; a += PGSIZE; pa += PGSIZE; return 0; 这里的pagetable实际上就是一片物理内存，walk函数通过一些位操作就可以得到虚拟地址对应的pte。 得到pte之后就可以将物理内存的信息写入pte中，同时pte设置我们想要的访问位，这样就完成了虚拟地址到物理地址的映射。 所以如果我们要申请内存，步骤是kalloc申请物理内存，随后确定对应的虚拟地址在哪，然后调用mappages映射并存入页表即可。 2.2.2 内核地址映射看一下内核初始化时如何创建页表并建立映射的： pagetable_tkvmmake(void) pagetable_t kpgtbl; kpgtbl = (pagetable_t) kalloc(); memset(kpgtbl, 0, PGSIZE); // map the trampoline for trap entry/exit to // the highest virtual address in the kernel. kvmmap(kpgtbl, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X); . . . // allocate and map a kernel stack for each process. proc_mapstacks(kpgtbl); return kpgtbl; 首先申请一片物理空间作为内核页表 对特定的地址空间进行映射，比如trampoline 对内核栈进行初始化和映射 proc_mapstacks内进行了内核栈的初始化： #define KSTACK(p) (TRAMPOLINE - (p)*2*PGSIZE - 3*PGSIZE)voidproc_mapstacks(pagetable_t kpgtbl) struct proc *p; for(p = proc; p proc[NPROC]; p++) char *pa = kalloc(); if(pa == 0) panic(kalloc); uint64 va = KSTACK((int) (p - proc)); kvmmap(kpgtbl, va, (uint64)pa, PGSIZE, PTE_R | PTE_W); 之前说过，xv6的PCB是一个静态数组proc，这里的内核栈直接为所有的PCB分配了内核栈，注意到宏KSTACK，地址之间的间隔就是上节所说的guard page。 2.2.3 开启paging上面的一系列操作都是没有页表的情况下完成的，也就是上面的内存操作都是物理地址。所以现在需要开启paging，把内核页表地址写入$satp寄存器即可，接下来的操作就都是虚拟地址了。 // and enable paging.voidkvminithart() // wait for any previous writes to the page table memory to finish. sfence_vma(); w_satp(MAKE_SATP(kernel_pagetable)); // flush stale entries from the TLB. sfence_vma(); 3. 用户进程与虚拟内存3.1 用户空间内存映射 相比于内核内存管理，用户进程的内存管理要复杂一点，复杂点主要在于： 所有进程都共享一个内核空间，即共用一张内核页表；但是每个进程都有自己的用户空间，所以每个进程都有单独的页表。 用户进程可以创建用户进程，用户进程也可以exec新的程序，这期间要对用户态页表进行正确的操作。 每个进程拥有独立的地址空间和映射关系，可以在这之上实现cow、lazy allocation等提升性能的技术，这需要对用户页表和page fault异常进行恰当的处理。 3.2 PCB了解了xv6的内存管理后，就可以看看PCB到底有哪些内容了，见中文注释： struct proc struct spinlock lock; // p-lock must be held when using these: enum procstate state; // 进程状态 void *chan; // sleep锁相关 int killed; // 被标记为杀死的进程 int xstate; // 进程退出时的状态 int pid; // 进程id // wait_lock must be held when using this: struct proc *parent; // 父进程PCB指针 // these are private to the process, so p-lock need not be held. uint64 kstack; // 内核栈虚拟地址 uint64 sz; // 分配给用户空间的物理内存大小 pagetable_t pagetable; // 用户态页表 struct trapframe *trapframe; // trapframe struct context context; // 进程调度的上下文相关 struct file *ofile[NOFILE]; // 进程文件描述父表 struct inode *cwd; // 当前目录的inode char name[16]; // 进程名称; 相对于真实世界os，xv6的PCB里仅有一些基本的数据，但是麻雀虽小五脏俱全。这里的PCB包括了trap、内存管理、进程调度、文件系统等内容，后面都会接触到，现在先对内存管理进程分析。 3.3 PCB的分配 xv6是一个静态数组来管理所有PCB，所以分配PCB时遍历数组找到一个可用的即可，若无可用则会返回nullptr。 随后将分配物理页作为进程的trapframe，trapframe之前有提过，在内核态用户态切换时保存寄存器。 建立一个用户态页表，并把trapframe映射到用户空间，同时trapframe的地址保存在了PCB中，因此内核和用户都能访问到trapframe。 最后关联一个内核栈地址，设置返回地址即可。 static struct proc*allocproc(void) struct proc *p; for(p = proc; p proc[NPROC]; p++) acquire(p-lock); if(p-state == UNUSED) goto found; else release(p-lock); return 0;found: p-pid = allocpid(); p-state = USED; // Allocate a trapframe page. if((p-trapframe = (struct trapframe *)kalloc()) == 0) freeproc(p); release(p-lock); return 0; // An empty user page table. p-pagetable = proc_pagetable(p); if(p-pagetable == 0) freeproc(p); release(p-lock); return 0; // Set up new context to start executing at forkret, // which returns to user space. memset(p-context, 0, sizeof(p-context)); p-context.ra = (uint64)forkret; p-context.sp = p-kstack + PGSIZE; return p; 3.4 init进程的创建父进程创建子进程，子进程将继承用户空间的映射（实际的物理内存是重新拷贝一份），子进程的栈也只要根据父进程的栈增长一部分即可。 而内核创建init进程，init没有父进程可以继承，所以内核要为它设置相关的数据，包括程序段的映射、程序计数器和用户栈指针的设置等。 可以看到，init程序段占据了一页映射在了页表中，同时p-sz也被置为了PGSIZE，标识用户空间已用一页内存。 voiduserinit(void) struct proc *p; p = allocproc(); initproc = p; // 程序段映射 uvmfirst(p-pagetable, initcode, sizeof(initcode)); p-sz = PGSIZE; // 设置$pc和$sp p-trapframe-epc = 0; p-trapframe-sp = PGSIZE; safestrcpy(p-name, initcode, sizeof(p-name)); p-cwd = namei(/); p-state = RUNNABLE; release(p-lock); 3.5 fork父进程创建子进程时，子进程将完全拷贝用户内存，但是页表中的映射保持不变，也就是说父进程子进程的相同的地址访问的内容相同，但是物理内存上是不同的。 具体实现上并不复杂，将sz内的页完全拷贝一份，映射到子进程页表中即可，注意映射的虚拟地址要与父进程相同。 // 遍历用户态分配的内存页for(i = 0; i sz; i += PGSIZE) if((pte = walk(old, i, 0)) == 0) panic(uvmcopy: pte should exist); if((*pte PTE_V) == 0) panic(uvmcopy: page not present); pa = PTE2PA(*pte); flags = PTE_FLAGS(*pte); if((mem = kalloc()) == 0) goto err; memmove(mem, (char*)pa, PGSIZE); if(mappages(new, i, PGSIZE, (uint64)mem, flags) != 0) kfree(mem); goto err; 3.6 page fault与应用page fault是trap的一种，当用户想要访问一个虚拟地址时，它对应的PTE访问位不满足条件，就会产生一个page fault，例如访问无效地址、读写限制等。 而os可以利用page fault实现一些很有用的技术： cow：后一节会提到，创建子进程时不立即拷贝所有用户内存。 lazy alloction：用户申请一大片内存空间，内核不立刻分配空间并映射，而是等到用户访问到这些内存产生page fault时才分配映射。 demand paging：在exec时，xv6是直接把可执行程序段都加载进内存，现实世界中有的程序段可能很大很大，所以只需要映射其中的一部分，等到有需要时再拷贝进内存并映射即可。 mmap：文件映射到内存，映射方法也可以采用lazy alloction的形式。 4. lab pgtbl4.1 speed syscall说的是可以利用用户空间的内存映射加速一些系统调用，比如现在想要获取当前进程pid，就要进内核态查PCB。但如果我们在创建进程时，将pid信息拷贝到用户空间并映射到特定的位置，这样在查pid时只要在用户空间完成即可，避免了切换内核态的开销。 实现方法是在fork时分配物理内存，用户空间添加映射即可，代码略。 4.2 Print a page table打印用户页表，根据PTE的定义三层遍历即可。 4.3 pgaccessint pgaccess(void *base, int len, void *mask); pgaccess实现的是打印进程用到了哪些物理页，一些GC程序可能会利用到这个来处理进行垃圾回收。 实现方法还是遍历页表，有一些不同的地方在于，pgaccess将页使用信息按位写在了mask里，但是pgaccess是用户发起的系统调用，也就是说mask是用户空间的内存，所以这里就要注意内核需要找到mask对应的物理地址再写入数据才能真正地把数据传递到用户空间。 5. Copy-on-Write5.1 原理xv6对fork的实现是，创建子进程时将父进程空间内的物理内存完全拷贝一份，并一一在子进程空间建立映射，但是这种做法非常低效。例如一个经典的操作是fork + exec，如果fork将内存全部拷贝完，随后执行exec时，用户空间将会被完全重置并建立新的映射，这种情况下是非常浪费CPU的。 故我们可以在fork时不立刻拷贝用户内存，而是对这些内存进行一些额外的标记。 如果页是只读的，那么不拷贝完全没有影响，父子进程都读同一段物理内存即可。 如果页是可写的，那么在fork时就需要在PTE中将它改为不可写，同时添加上COW标志。随后如果进程想写COW页，那么此时可以将这一页真正地拷贝：分配物理内存并在页表中建立映射，随后该进程写这一片新拷贝的内存即可。 5.2 实现方法5.2.1 forkfork时只需要改变用户内存的拷贝逻辑即可，将用户页的PTE移除写标识，添加PTE_C的标志，用于后续的page fault时判断。 for (i = 0; i sz; i += PGSIZE) if ((pte = walk(old, i, 0)) == 0) panic(uvmcopy: pte should exist); if((*pte PTE_V) == 0) panic(uvmcopy: page not present); if((*pte PTE_W) != 0) *pte = (*pte (~PTE_W)) | PTE_C; // if PTE_C or PTE_R, copy pa = PTE2PA(*pte); if (mappages(new, i, PGSIZE, pa, PTE_FLAGS(*pte)) != 0) goto err; // 物理页的引用计数修改 princr(pa); 考虑这么一个场景：父进程创建了两个子进程，他们都共享一个可写页A。第一个子进程写A，执行了写拷贝；第二个子进程写A，又执行了写拷贝；此时父进程再要写A，就可以直接在当前的物理地址写就可以了，因为此时没有进程跟他共享A了。 为了实现这个逻辑，就需要添加一个数据结构，对所有物理页的引用次数进行统计，当COW页的引用次数为1时，就不需要再进行写时复制了，直接写即可。 5.2.2 trapfork里修改的是对内存拷贝的处理，而COW的写时复制需要靠page fault来实现，即处理对应的trap，包括以下几步： 在trap处理中捕获写page fault，并得到触发page fault的虚拟地址。 在用户进程页表中检查地址对应的PTE，如果有COW位，则进行写时复制的判断。 判断当前物理页的引用次数，大于1才进行复制，否则直接修改PTE的写位即可 if (r_scause() == 15) // stval是触发的虚拟地址 uint64 vm = r_stval(); if (vm = MAXVA) exit(-1); pte_t* pte; if ((pte = walk(p-pagetable, vm, 0)) == 0) exit(-1); if ((*pte PTE_C) == 0) exit(-1); // 物理地址 uint64 pa = PTE2PA(*pte); // 引用次数进行判断 if (prget(pa) == 1) *pte = (*pte (~PTE_C)) | PTE_W; else if (prget(pa) = 1) // 开始实际的页拷贝 uint perm = (PTE_FLAGS(*pte) (~PTE_C)) | PTE_W; char *mem; if ((mem = kalloc()) == 0) exit(-1); memmove(mem, (char *)pa, PGSIZE); uvmunmap(p-pagetable, PGROUNDDOWN(vm), 1, 1); if(mappages(p-pagetable, PGROUNDDOWN(vm), PGSIZE, (uint64)mem, perm) != 0) kfree(mem); exit(-1); else panic(paref cnt 0); 6. 总结对比真实世界的os，xv6的内存管理要简化了很多很多： xv6在内核态是，虚拟地址和物理地址是直接映射的，这为我们实现功能简化了很多操作。 xv6的物理页并没有置换功能，现实中的os是实现了物理页置换功能的。 xv6的内核没有malloc，所以申请栈外内存都是很粗糙地直接取一个物理页。 虽然有所简化，但是收获还是很多的～"},{"title":"6.s081 traps","path":"/2025/01/19/6-s081-traps/","content":"1. 关于trapxv6中的trap分为三种：系统调用、程序异常、硬件中断。其中系统调用trap在上一篇已经详细介绍过了：https://zzxy1999.github.io/2025/01/15/6-s081-syscall/ ，其他的trap处理也是大同小异（硬件的具体处理有所不同），所以直接进入lab。 2. lab traps2.1 RISC-V assembly读汇编答问题，略。 2.2 Backtrace要求在内核添加一个backtrace函数，同时添加在panic函数中，这样每次panic都会由函数调用信息。 2.2.1 函数调用函数调用栈中保存着返回地址，而$fp寄存器保存着当前调用栈的起始位置，所以可以很容易得到当前函数的返回地址。但是backtrace是需要递归地打印调用关系的，所以还需要知道当前函数的调用者的栈信息。所幸的是当前栈的栈底就连接着调用者的栈顶，而调用者的fp寄存器就保存在它的栈底的特定位置，所以我们可以得到调用者的栈信息并打印返回地址信息。按照这个思路，就可以递归地打印出函数的调用关系，直到用户空间栈边界。代码非常简短，递归调用即可。 voidbacktrace() printf(backtarce: ); uint64 pre = r_fp(); uint64 pgd = PGROUNDDOWN(pre); for(;;) uint64 ret_addr = *(((uint64*)pre) - 1); printf(%p , ret_addr); pre = *(((uint64*)pre) - 2); if (PGROUNDDOWN(pre) != pgd) break; 后面开始介绍比较有难度的optional challenge。 2.2.2 addr2line值得注意的是上面的backtrace打印的是指令地址，而平时用的backtrace打印的都是程序行数这种信息，所以我们还需要把指令地址与程序行数联系起来。一种非常简单的方法，就是用addr2line程序，输入指令地址就可以得到程序行数了；但是这里作为一个完整的backtrace，最好还是自己做一个addr2line在backtrace中才好。 2.3.3 elf首先梳理一下目标，现在已知指令地址，需要知道对应的代码行数，这些debug信息其实在可执行文件中都有保存，所以需要做的就是对elf进行解析。先贴个链接，里面说明了elf文件的结构定义（看起来比较费劲）：https://refspecs.linuxfoundation.org/elf/gabi4+/ch4.eheader.html手画个elf结构图：我们想要的地址信息就都在.debug_line里。 2.3.4 elf头下面列出了几个elf头比较重要的信息，我们需要的是根据头信息得到几个section信息。举个例子，elf文件的shoff处就是第一个section头的位置，我们只要按section头的结构读取这里的数据，就可以得到第一个section头；如果需要第二个section头，只需要读文件偏移shoff + sizeof(section header)即可。 // File headerstruct elfhdr uint magic; // must equal ELF_MAGIC uchar elf[12]; ushort type; ushort machine; uint version; uint64 entry; uint64 phoff; uint64 shoff; // section头所在位置 uint flags; ushort ehsize; ushort phentsize; ushort phnum; ushort shentsize; ushort shnum; // section的数目 ushort shstrndx; // section名称表所在位置; 2.3.5 section头section头里的信息如下： typedef struct uint name; // section的名称 uint type; // section类型 uint64 flags; uint64 addr; uint64 off; // section的位置 uint64 size; // section的大小 uint link; uint info; uint64 addralign; uint64 entsize; shdr; 综合上面的信息，尝试打印一个elf文件的elf头和各个section头的信息可以看到section头的name都是数字，其实他们标识的是自己的名字在名称表中的位置，名称表由若干个字符串拼接而成。名称表也是一个section，相应的它也有section头，也就是说上面的一系列section头中有我们现在需要的名称表，接下来的任务是找出谁是名称表。名称表的type是3,但是可以看到有两个section头的type是3，但是这里有两个3。回顾一下elf头，里面的shstrndx为16，它其实就指出了section名称表的位置，也就是图中最后一个条目。 2.3.6 shstrtab根据最后一个section头的信息，可以打印出section内的内容：配合着section头内的name再打印一边section头信息可以看到非常清晰了： 2.3.7 .debug_line经过一步步处理，我们已经知道debug_line在哪个段了，接下来需要对debug_line内的内容进行处理。elf中的debug_line并非简单的地址-行号的映射，它实际上是一个按字节处理的状态流。想要读取其中的信息，需要构建一个简易的状态机，保存行号、地址、文件名等信息，逐一读取debug_line中的字节，来驱使状态机得到一个个地址-行号的映射，得到了所有的映射后，将我们backtrace得到的地址利用二分查找找到对应行号即可。 2.3.8 exec与elf既然elf都看到这了，就顺便看一下跟exec相关的代码： for(i=0, off=elf.phoff; ielf.phnum; i++, off+=sizeof(ph)) if(readi(ip, 0, (uint64)ph, off, sizeof(ph)) != sizeof(ph)) goto bad; if(ph.type != ELF_PROG_LOAD) continue; if(ph.memsz ph.filesz) goto bad; if(ph.vaddr + ph.memsz ph.vaddr) goto bad; if(ph.vaddr % PGSIZE != 0) goto bad; uint64 sz1; if((sz1 = uvmalloc(pagetable, sz, ph.vaddr + ph.memsz, flags2perm(ph.flags))) == 0) goto bad; sz = sz1; if(loadseg(pagetable, ph.vaddr, ip, ph.off, ph.filesz) 0) goto bad; 学习了elf文件和各个section的结构后，这段代码的含义就清晰很多了：exec从elf特定的偏移位置读取文件，读取的数据就是可执行的指令流，把这些指令流拷贝到用户空间，最后把用户的$pc指向指令流的开始处即可。 2.3 Alarm这部分要实现的是一对Alarm系统调用，实现的功能是定时器触发一定次数时执行之前设置好的函数。 int sigalarm(int ticks, void (*handler)());int sigreturn(void);voidhandler() count = count + 1; printf(alarm! ); sigreturn();voidtest0() sigalarm(2, handler); // wait 2.3.1 定时器的处理定时器的处理比较直观：tick也是一种trap，所以在处理trap时可以以进程维度对tick进行计数，当次数满足用户要求是，调用用户传进的函数即可。 2.3.2 控制流的传递这里要考虑一下的是控制流该如何传递： 用户态下tick触发中断 内核态的trap处理，如果tick次数达到，进入handler函数 handler函数调用sigreturn进入内核 sigreturn返回到1处被中断的位置 在syscall里提过，用户态内核态的切换，寄存器保存在了trapframe中，所以我们可以通过修改trapframe中保存的pc来改变控制流： 1–2: 把trapframe中的pc改成handler的地址，再返回；需要注意的handler在用户态，进入handler原有的trapframe会被破坏，所以需要在PCB内保存一下原始的trapframe。 2–3: handler的最后是sigreturn，再次进入内核。 3–4: 在sigreturn内，把trapframe替换为之前保存的trapframe，这样再返回用户态就能顺利返回到1的中断处了。 3. 总结两个lab都很有意思，一个是理解了backtrace的原理，顺便学习了elf文件的内部结构；另一个学习到了如何利用改变寄存器控制程序流的走向。"},{"title":"6.s081 syscall","path":"/2025/01/15/6-s081-syscall/","content":"0. 前言6.s081是MIT开设的os课，前段时间把所有lab都写完了，最近有空便重新阅读一下xv6的源码，结合lab记录一下。课程链接在这https://pdos.csail.mit.edu/6.828/2022/个人的lab实现在这https://github.com/Zzxy1999/6.s081 1. xv6的启动1.1 从开机到加载操作系统电源打开后，计算机会从ROM中加载一段叫boot loader的程序到内存，boot loader的主要作用是把内核文件（elf）加载进内存。内核大多是C代码，而C代码的执行需要有栈，所以内核的第一段程序用汇编写成即entry.s，它的作用主要是设置一段内存作为内核栈，移动$sp到栈顶，再把控制交给内核的C代码。 _entry:\t# set up a stack for C.\t# stack0 is declared in start.c,\t# with a 4096-byte stack per CPU.\t# sp = stack0 + (hartid * 4096)\tla sp, stack0\tli a0, 1024*4\tcsrr a1, mhartid\taddi a1, a1, 1\tmul a0, a0, a1\tadd sp, sp, a0\t# jump to start() in start.c\tcall start 1.2 start现在有了C代码的栈空间，可以执行内核的C代码了，第一段C代码start位于start.c中，start执行了一些机器级的初始化，例如时钟中断初始化等，随后就要进入操作系统的main函数了。 void main();/ entry.S jumps here in machine mode on stack0.voidstart()\t... // set M Exception Program Counter to main, for mret. // requires gcc -mcmodel=medany w_mepc((uint64)main); // switch to supervisor mode and jump to main(). asm volatile(mret); 汇编代码里可以轻易的jump到想去的函数位置，而C程序中函数返回将返回到它的调用者处。而start想去的是main函数，所以就需要对当前的返回地址做修改，将它改成main的起始地址，随后再进行ret，就可以顺利地跳转到main函数位置了。 1.3 main操作系统的main函数包含很多os相关的初始化函数，包括了设备、内核页表、trap、fs等相关数据结构。初始化结束后就要开始生成第一个用户进程，也就是常说的init进程了。 voidmain() if(cpuid() == 0) consoleinit(); printfinit(); printf( ); printf(xv6 kernel is booting ); printf( ); kinit(); // physical page allocator kvminit(); // create kernel page table kvminithart(); // turn on paging procinit(); // process table trapinit(); // trap vectors trapinithart(); // install kernel trap vector plicinit(); // set up interrupt controller plicinithart(); // ask PLIC for device interrupts binit(); // buffer cache iinit(); // inode table fileinit(); // file table virtio_disk_init(); // emulated hard disk userinit(); // first user process __sync_synchronize(); started = 1; else while(started == 0) ; __sync_synchronize(); printf(hart %d starting , cpuid()); kvminithart(); // turn on paging trapinithart(); // install kernel trap vector plicinithart(); // ask PLIC for device interrupts scheduler(); 1.4 userinitinit和内核程序不同，它运行在用户态，它有着用户进程页表、父子进程、进程状态等多种属性，userinit要做的就是创建这样一个进程，同时对它进行恰当的初始化，这里的初始化可以分为几个部分： 进程控制块：xv6的PCB是一个静态数组，内核直接调用allocproc获取一个PCB作为init进程的PCB。 页表：由于init进程没有真正意义上的父进程，所以页表就没法继承，就得内核帮它构建映射。uvmfirst的作用就是把init这个用户程序映射到这个进程的页表中，位置就在用户空间0地址处。 栈：用户空间得有自己的栈，而栈的位置设置在了PGSIZE处，也就是init用户程序的下一页的位置。 voiduserinit(void) struct proc *p; p = allocproc(); initproc = p; // allocate one user page and copy initcodes instructions // and data into it. uvmfirst(p-pagetable, initcode, sizeof(initcode)); p-sz = PGSIZE; // prepare for the very first return from kernel to user. p-trapframe-epc = 0; // user program counter p-trapframe-sp = PGSIZE; // user stack pointer safestrcpy(p-name, initcode, sizeof(p-name)); p-cwd = namei(/); p-state = RUNNABLE; release(p-lock); 一些额外的讨论： PCB是一个静态数组是因为内核没有动态内存分配模块（如果内核要用内存，要不在内核栈分配，要不就是直接要一个物理页来进行操作），所以只能用静态数组，系统内的进程数也就有了限制。 用户栈直接被映射在了PGSIZE处，随后所有的进程栈空间都在这个基础上，遵循C语言函数调用栈规则即可。 可以看到这里的$pc获得$sp都不是直接设置寄存器，而是设置在了trapframe中，这个trapframe相当于保存的被调用者寄存器，当userinit返回时，$pc将会被正确地置于init程序处。 1.5 initinit就是运行在用户态的用户程序，大概执行的就是打开console重定向，创建第二个子进程让它来执行shell程序，之后再进入永不停止的for循环即可。 intmain(void) // open console dup // fork and exec shell // while 至此，xv6完成了电源启动到init进程创建的全过程。 2. lab util第一个lab用系统调用在xv6内写几个用户程序，例如find xargs这种，不再赘述。 3. 关于系统调用以os的设计者视角来看，肯定不希望用户直接对内核数据、设备硬件等资源直接进行操作，但是用户程序往往需要用到这些资源（比如简单的printf）。所以os就提供了一系列接口，帮用户来实现这些内核级的操作，这些接口就是系统调用。 3.1 系统调用的整体流程xv6中的系统调用流程大致为： 用户发起系统调用 控制转移到uservec（trampoline.s）为进入内核做准备 进入内核的usertrap函数，判断是否为系统调用 根据系统调用表查找相应的处理函数 处理函数读取参数并完成相应的功能 完成要求后做一些退出准备，随后返回用户空间 3.2 trap相关的寄存器系统调用是一种用户级trap，riscv中有很多寄存器跟trap的处理关系密切，下面列几个比较重要。 stvec: 里面记录着处理trap的入口地址 sepc: 进入内核后，返回到用户空间的地址 scause: trap原因，编号表示 sstatus: trap由内核引发还是用户引发 下面用gdb简单的验证一下stvec寄存器的地址在哪。 gdb停在了内核空间，打印stvec寄存器的值，为0x80005120。 看看0x80005120到底是哪，发现是kernelvec的地址，在kernelvec.S中，是处理内核trap的位置。 原因是stvec会在用户空间和内核空间变化，在内核空间看到的stvec自然是处理内核trap的起始位置。下面执行一个用户程序，进入用户空间看看stvec寄存器的值。 简单执行一个ls程序，并在执行系统调用open前，打印stvec的值0x3ffffff000。 断点到0x3ffffff000处，发现这里是trampoline.S的地方，符合预期。 3.3 trapframe从用户空间切换到内核空间，有很多寄存器都要保存下来，保存的位置在用户地址TRAPFRAME处，这些寄存器以trapframe这个数据结构的形式保存，这些寄存器中有些在系统调用中非常重要： a7寄存器保存了系统调用号 a1-a6保存了各个系统调用参数 a0则是系统调用返回值 内核想要知道系统调用号、系统调用参数，就需要知道这些寄存器的具体值，但现在的问题是trapframe映射在了用户空间，内核该怎么读取？事实上从init进程来看，内核在创建它时，就为它的页表做了一部分映射，其中就包括trapframe，所以内核是知道trapframe的物理地址的，它将其保存在了PCB中。而init所有的子进程在继承页表时，trapframe的虚拟地址不变，物理地址会变，所以内核会将PCB中记录一并修改。所以，内核只要获取PCB中的trapframe物理地址，就可以轻松地访问用户态下保存的寄存器了。 3.4 用户空间的工作 用户发起系统调用后会执行一个ecall指令，ecall会把控制流转移到uservec函数（在trapolin.S中，前面说道stvec保存了这个函数的地址）。 uservec保存用户空间下的寄存器到trapframe中，同时设置栈、页表等，为进入内核空间做准备，随后将控制流转移到内核的usertrap处。 3.5 usertrap进入内核后首先到usertrap这里，这个函数后面的lab会经常打交道。了解了上面说的种种寄存器后，对系统调用的处理也就水到渠成了。 sstatus判断是否是用户态引发的trap。 设置返回用户态的pc，这里注意到如果是系统调用，pc+4即返回到下一条指令。 判断scause是否为8，8表示这个trap是用户引发的系统调用，随后进入syscall函数来专门处理系统调用。 if((r_sstatus() SSTATUS_SPP) != 0) panic(usertrap: not from user mode); // send interrupts and exceptions to kerneltrap(), // since were now in the kernel. w_stvec((uint64)kernelvec); struct proc *p = myproc(); // save user program counter. p-trapframe-epc = r_sepc(); if(r_scause() == 8) // system call if(killed(p)) exit(-1); // sepc points to the ecall instruction, // but we want to return to the next instruction. p-trapframe-epc += 4; // an interrupt will change sepc, scause, and sstatus, // so enable only now that were done with those registers. intr_on(); syscall(); 3.6 syscall代码非常简短，获取系统调用号，在a7寄存器，然后在syscalls这个系统调用表中查找对应的函数执行即可。 // 即系统调用表static uint64 (*syscalls[])(void) = [SYS_fork] sys_fork, [SYS_exit] sys_exit, [SYS_wait] sys_wait, ...voidsyscall(void) int num; struct proc *p = myproc(); num = p-trapframe-a7; // num = *(int*)0; if(num 0 num NELEM(syscalls) syscalls[num]) // Use num to lookup the system call function for num, call it, // and store its return value in p-trapframe-a0 p-trapframe-a0 = syscalls[num](); else printf(%d %s: unknown sys call %d , p-pid, p-name, num); p-trapframe-a0 = -1; 3.5 sys_sleep以sys_sleep系统调用为例，这里唯一需要注意的是参数的获取，前面说过用户寄存器有系统调用的参数，他们都保存在了trapframe中，利用封装的argint函数就可以获得对应的参数了。 uint64sys_sleep(void) int n; uint ticks0; // 封装的函数，内部直接获取trapframe内保存的用户寄存器 argint(0, n); if(n 0) n = 0; acquire(tickslock); ticks0 = ticks; while(ticks - ticks0 n) if(killed(myproc())) release(tickslock); return -1; sleep(ticks, tickslock); release(tickslock); return 0; 需要额外说的是，如果系统调用参数是一个字符串，那么寄存器中获得的是这个字符串的地址，而这个地址是用户态地址，内核想要获得字符串的值就要在内核与用户空间进行数据的传输，这里的具体过程pagetable再讨论。 4. lab syscall这个lab分为三个部分，只要熟悉了xv6的系统调用过程，做起来并不困难。 4.1 gdb使用gdb打印几个值回答问题即可，注意gdb调试内核的方法，可以通过file在用户程序和内核程序间跳转。 4.2 syscall trace实现一个功能，每次执行系统调用时，在stdout上打印出系统调用名和返回值，在上一章所示的syscall函数中加个printf即可，不贴代码了。 4.3 sysinfo实现一个新的系统调用，功能是打印内核进程状态，这里梳理一下怎么在xv6内添加系统调用。 在用户空间的系统调用头文件中加sysinfo的声明。 想要系统调用进入内核态，需要执行ecall指令。xv6提供了usys.pl，只需要在内部写入系统调用名称，make时将会自动生成几行对应的ecall指令。 有了ecall之后，控制流就可以正确地传递到内核了，接下来需要在系统调用表中注册内核的sysinfo函数，用来执行系统调用。 最后一步就是实现sysinfo函数了，这是最简单的一部分，遍历一遍内核中进程表统计一下进程，通过trapframe中保存的寄存器返回结果即可。 5. 总结温故而知新"},{"title":"Blog from CSDN","path":"/2025/01/15/Blog-from-CSDN/","content":"https://blog.csdn.net/qq_40955029?spm=1000.2115.3001.5343"}]