[{"title":"6.s081 syscall","path":"/2025/01/15/6-s081-syscall/","content":"前言6.s081是MIT开设的os课，前段时间把所有lab都写完了，最近有空便结合xv6记录一下收获～课程链接在这https://pdos.csail.mit.edu/6.828/2022/个人的lab实现在这https://github.com/Zzxy1999/6.s081 1. xv6如何启动的？1.1 boot loader到entry电源打开后，计算机将会执行一段叫boot loader的程序，这段程序存储在ROM里，把它加载进内存后，CPU便开始执行。boot load会从磁盘固定位置上加载内核的一小段程序entry.S。 _entry:\t# set up a stack for C.\t# stack0 is declared in start.c,\t# with a 4096-byte stack per CPU.\t# sp = stack0 + (hartid * 4096)\tla sp, stack0\tli a0, 1024*4\tcsrr a1, mhartid\taddi a1, a1, 1\tmul a0, a0, a1\tadd sp, sp, a0\t# jump to start() in start.c\tcall start entry做的工作包括了初始化一个C栈，随后在+4096的位置设置相应的栈指针，有了内核栈之后，entry将程序控制权转到了C代码start.c处。 1.2 startvoid main();/ entry.S jumps here in machine mode on stack0.voidstart()\t... // set M Exception Program Counter to main, for mret. // requires gcc -mcmodel=medany w_mepc((uint64)main); // switch to supervisor mode and jump to main(). asm volatile(mret); start内部做了一些初始化工作，随后要正式进入os的main函数了。注意到start并不是被函数调用的，而是直接修改pc跳转过来的。这里要把控制权从start转移到main，用的方法是设置函数结束后ret的地址到main的起始地址，随后执行ret指令，这样就将控制权转移到了main处。 1.3 mainvoidmain() if(cpuid() == 0) consoleinit(); printfinit(); printf( ); printf(xv6 kernel is booting ); printf( ); kinit(); // physical page allocator kvminit(); // create kernel page table kvminithart(); // turn on paging procinit(); // process table trapinit(); // trap vectors trapinithart(); // install kernel trap vector plicinit(); // set up interrupt controller plicinithart(); // ask PLIC for device interrupts binit(); // buffer cache iinit(); // inode table fileinit(); // file table virtio_disk_init(); // emulated hard disk userinit(); // first user process __sync_synchronize(); started = 1; else while(started == 0) ; __sync_synchronize(); printf(hart %d starting , cpuid()); kvminithart(); // turn on paging trapinithart(); // install kernel trap vector plicinithart(); // ask PLIC for device interrupts scheduler(); 可以看到，main内执行了一大堆os的初始化操作，随后的scheduler会正式开启cpu的调度。各个模块的初始化后面的lab会又提其，这里重点看下userinit，即第一个用户进程，init进程。 1.4 userinitvoiduserinit(void) struct proc *p; p = allocproc(); initproc = p; // allocate one user page and copy initcodes instructions // and data into it. uvmfirst(p-pagetable, initcode, sizeof(initcode)); p-sz = PGSIZE; // prepare for the very first return from kernel to user. p-trapframe-epc = 0; // user program counter p-trapframe-sp = PGSIZE; // user stack pointer safestrcpy(p-name, initcode, sizeof(p-name)); p-cwd = namei(/); p-state = RUNNABLE; release(p-lock); 这里看到，正常的进程都是由父进程创建的，他们的栈、pc等都和父进程有关。而init进程是由内核直接创建，所以需要内核帮它设置这些必要的参数。可以看到它的栈被内核设置在了PGSIZE处，它的pc被设置在了0地址处（这里并非简单的设置，和后面的trap有关）。而0地址有一段汇编代码initcode.S，这里不再贴出，其具体作用是执行exec系统调用，执行init程序。 1.5 initintmain(void) int pid, wpid; if(open(console, O_RDWR) 0) mknod(console, CONSOLE, 0); open(console, O_RDWR); dup(0); // stdout dup(0); // stderr for(;;) printf(init: starting sh ); pid = fork(); if(pid 0) printf(init: fork failed ); exit(1); if(pid == 0) exec(sh, argv); printf(init: exec sh failed ); exit(1); for(;;) // this call to wait() returns if the shell exits, // or if a parentless process exits. wpid = wait((int *) 0); if(wpid == pid) // the shell exited; restart it. break; else if(wpid 0) printf(init: wait returned an error ); exit(1); else // it was a parentless process; do nothing. 到init这里起始就好懂不少，打开console，然后重定向。随后init自己创建一个进程，这个进程就是第二个用户进程，执行sh程序，也就是一个shell，随后就是无尽的循环了～到此xv6启动完毕，同时提供给了用户一个终端进行操作。 2. lab util先放链接https://pdos.csail.mit.edu/6.828/2022/labs/util.html第一个lab很简单，task1是在qemu内启动xv6,执行几个命令行就ok了；task2是编写几个用户程序，比如find xargs这种，xv6本身就提供了不少系统调用，所以就直接用系统调用函数写就ok了，这里不再赘述了。 3. 系统调用是怎么执行的？为什么要有系统调用？原因是用户需要执行一个操作，而这些操作往往要涉及os内部核心数据，甚至需要接触硬件设备。这些操作交由用户来实现肯定是不放心的，故这些操作必须要在内核完成，即借助系统调用，让内核帮自己完成这些操作。下面梳理下xv6中，系统调用的过程。 3.1 系统调用的整体流程xv6中的系统调用流程大致为： 用户发起系统调用 控制转移到uservec（trampoline.S）为进入内核做准备 进入内核的usertrap函数，判断是否为系统调用 根据系统调用表查找相应的处理函数 处理函数读取参数实际运行系统调用 做一系列准备处理后返回用户空间 3.2 trap相关的寄存器系统调用也是一种用户级trap，riscv中有很多寄存器跟trap的处理关系密切，下面列几个比较重要。 stvec: 里面记录着处理trap的地址 sepc: 进入内核后，返回到用户空间的地址 scause: trap原因 sstatus: trap由内核引发还是用户引发 下面用gdb简单的验证一下stvec寄存器的地址在哪。 gdb停在了内核空间，打印stvec寄存器的值，为0x80005120。 看看0x80005120到底是哪，发现是kernelvec的地址，在kernelvec.S中，是处理内核trap的位置。 原因是stvec会在用户空间和内核空间变化，在内核空间看到的stvec自然是处理内核trap的起始位置。下面执行一个用户程序，进入用户空间看看stvec寄存器的值。 简单执行一个ls程序，并在执行系统调用open前，打印stvec的值0x3ffffff000。 断点到0x3ffffff000处，发现这里是trampoline.S的地方，符合预期。 3.3 trapframe从用户空间切换到内核空间，所有的寄存器都要保存下来，保存的位置在虚拟地址TRAPFRAME处，其数据结构为trapframe。trapframe内的寄存器在执行系统调用时很有用： a7寄存器保存了系统调用号 a1-a6保存了各个系统调用参数 a0则是系统调用返回值 内核通过读or写这些保存的寄存器，达到了在用户空间和内核空间传递参数和返回值的目的。trapframe是映射在了用户空间的，所以在用户空间准备时将寄存器写入其中没问题。而内核想要读这些保存的寄存器，是不能走用户空间的页表的，为了正常访问trapframe，在进程创建初期，内核就记录了trapframe映射的物理地址，所以内核后期可以正常读写它。 3.4 用户空间的工作 用户发起系统调用后会执行一个ecall指令，ecall会把控制流转移到uservec函数（在trapolin.S中，前面说道stvec保存了这个函数的地址）。 uservec保存用户空间下的寄存器到trapframe中，同时设置栈页表等，为进入内核空间做准备，随后将控制流转移到内核的usertrap处。 3.5 usertrap进入内核后首先到usertrap这里，这个函数后面的lab会经常打交道。了解了上面说的种种寄存器后，对系统调用的处理也就水到渠成了。 sstatus判断是否是用户态引发的trap。 设置返回用户态的pc，这里注意到如果是系统调用，pc+4即返回到下一条指令。 判断scause是否为8，8表示这个trap是用户引发的系统调用，随后进入syscall函数来专门处理系统调用。 if((r_sstatus() SSTATUS_SPP) != 0) panic(usertrap: not from user mode); // send interrupts and exceptions to kerneltrap(), // since were now in the kernel. w_stvec((uint64)kernelvec); struct proc *p = myproc(); // save user program counter. p-trapframe-epc = r_sepc(); if(r_scause() == 8) // system call if(killed(p)) exit(-1); // sepc points to the ecall instruction, // but we want to return to the next instruction. p-trapframe-epc += 4; // an interrupt will change sepc, scause, and sstatus, // so enable only now that were done with those registers. intr_on(); syscall(); 3.6 syscall代码非常简短，获取系统调用号，在a7寄存器，然后在syscalls这个系统调用表中查找对应的函数执行即可。 // 即系统调用表static uint64 (*syscalls[])(void) = [SYS_fork] sys_fork, [SYS_exit] sys_exit, [SYS_wait] sys_wait, ...voidsyscall(void) int num; struct proc *p = myproc(); num = p-trapframe-a7; // num = *(int*)0; if(num 0 num NELEM(syscalls) syscalls[num]) // Use num to lookup the system call function for num, call it, // and store its return value in p-trapframe-a0 p-trapframe-a0 = syscalls[num](); else printf(%d %s: unknown sys call %d , p-pid, p-name, num); p-trapframe-a0 = -1; 3.5 sys_sleepuint64sys_sleep(void) int n; uint ticks0; // 封装的函数，内部直接获取trapframe内保存的用户寄存器 argint(0, n); if(n 0) n = 0; acquire(tickslock); ticks0 = ticks; while(ticks - ticks0 n) if(killed(myproc())) release(tickslock); return -1; sleep(ticks, tickslock); release(tickslock); return 0; 再进最后一步，内核来到sleep系统调用的处理函数。sleep系统调用是有参数的，这个参数由trapframe中保存的用户寄存器来，获得所有参数后，再调用各种内核函数实现用户要求的功能即可。 4. lab syscall先放链接https://pdos.csail.mit.edu/6.828/2022/labs/syscall.html弄清楚系统调用的全过程，几个lab就非常简单了。 4.1 gdb使用gdb打印几个值回答问题即可，注意gdb调试内核的方法，可以通过file在用户程序和内核程序间跳转。https://pdos.csail.mit.edu/6.828/2022/labs/gdb.html 4.2 syscall trace实现一个功能，每次执行系统调用时，在stdout上打印出系统调用名和返回值，在上一章所示的syscall函数中加个printf即可，不贴代码了。 4.3 sysinfo实现一个新的系统调用，功能是打印内核进程状态，这里梳理一下怎么在xv6内添加系统调用。 在用户空间的系统调用头文件中加sysinfo的声明。 想要系统调用进入内核态，需要执行ecall指令。xv6提供了usys.pl，只需要在内部写入系统调用名称，make时将会自动生成几行对应的ecall指令。 有了ecall之后，控制流就可以正确地传递到内核了，接下来需要在系统调用表中注册内核的sysinfo函数，用来执行系统调用。 最后一步就是实现sysinfo函数了，这是最简单的一部分，遍历一遍内核中进程表统计一下进程，通过trapframe中保存的寄存器返回结果即可。 5. 总结前两个lab完成起来并不难，但是包含的内容非常重要，尤其是系统的trap的处理，gdb调试用户程序和内核的方法，后面会用的非常频繁。几个月前刚接触xv6做这两个lab也是磕磕绊绊，现在回头看下来通畅很多，确实是温故而知新～"},{"title":"CSAPP Lab Record","path":"/2025/01/15/CSAPP-Lab-Record/","content":"以前写的CSAPP Lab记录https://blog.csdn.net/qq_40955029?spm=1000.2115.3001.5343"}]